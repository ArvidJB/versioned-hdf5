{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='home'></a>\n",
    "\n",
    "# Growth of File Sizes for Versioned-HDF5 Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these tests, we have generated `.h5` data files using the `generate_data_deterministic.py` script from the [VersionedHDF5 repository](https://github.com/Quansight/versioned-hdf5), using the standard options ([see details here](#standard))\n",
    "\n",
    "We performed the following tests:\n",
    "1. [Test Large Fraction Changes Sparse](#test1)\n",
    "2. [Test Mostly Appends Sparse](#test2)\n",
    "3. [Test Small Fraction Changes Sparse](#test3)\n",
    "4. [Test Mostly Appends Dense](#test4)\n",
    "\n",
    "**These tests were last run on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.utcnow(), \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the generated test files is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/melissa/projects/versioned-hdf5/analysis\" # change this as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "from versioned_hdf5 import VersionedHDF5File\n",
    "from generate_data_deterministic import TestVersionedDatasetPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary code to format file sizes \n",
    "def format_size(size):\n",
    "    suffixes = ['B', 'KB', 'MB', 'GB']\n",
    "    i = 0\n",
    "    while size >= 1024 and i < len(suffixes)-1:\n",
    "        size = size/1024\n",
    "        i += 1\n",
    "    return f\"{size:.2f} {suffixes[i]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test1'></a>\n",
    "\n",
    "# Test 1: Large fraction changes (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_large_fraction_changes_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tested the following numbers of versions (or transactions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_1 = [50, 100, 500, 1000, 5000, 10000]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify `num_transactions_1` for the desired tests. **Please keep in mind that file sizes can become very large for large numbers of transactions (above 5000 transactions).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_1 = [50, 100, 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chunk size parameter, we have tested chunk sizes of $2^8, 2^{10}, 2^{12}$ and $2^{14}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponents = [8, 10, 12, 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose desired compression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files and set up dictionary with test info.\n",
    "tests_complete = []\n",
    "for c in compression:\n",
    "    for p in exponents:\n",
    "        for n in num_transactions_1:\n",
    "            chunk_size = 2**p\n",
    "            name = f\"{testname}_{n}_{p}_{c}\"\n",
    "            filename = os.path.join(path, f\"{name}.h5\")\n",
    "            print(\"File with\\n\" \\\n",
    "                  f\"- {n} transactions\\n\" \\\n",
    "                  f\"- chunk size 2**{p}\\n\"\\\n",
    "                  f\"- compression filter {c}\")\n",
    "            try:\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "                print(\"already exists - unable to compute creation time.\")\n",
    "                t = 0\n",
    "            except:\n",
    "                print(\"not available. Creating new file.\")\n",
    "                t0 = time.time()\n",
    "                TestVersionedDatasetPerformance().test_large_fraction_changes_sparse(n, name, chunk_size, c)\n",
    "                t = time.time()-t0\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "            data = VersionedHDF5File(h5pyfile)\n",
    "            tests_complete.append(dict(num_transactions=n, \n",
    "                                       chunk_size=chunk_size, \n",
    "                                       filename=filename, \n",
    "                                       h5pyfile=h5pyfile, \n",
    "                                       data=data, \n",
    "                                       t_write=t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of versions v. File size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by analyzing how the `.h5` file sizes grow as the number of versions grows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for test in tests_complete:\n",
    "    test['size'] = os.path.getsize(test['filename'])\n",
    "    test['size_label'] = format_size(test['size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the array size also grows as the number of versions grows, since each transaction is changing the original arrays by adding, deleting and changing values in the original arrays. In order to compute a (naive) theoretical lower bound on the file size, we'll compute how much space each version should take. Keep in mind there is redundant data as some of it is not changed during the staging of a new version but it is still being stored. In this example, we start with three arrays with 5000 elements (2 integer arrays and one float), and in the end we have the following array sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(num_transactions_1)\n",
    "# We take just the last n items in test_complete because usually \n",
    "# they will be the fastest to read/write \n",
    "for test in tests_complete[-n:]:\n",
    "    lengths = []\n",
    "    total_size = 0\n",
    "    for vname in test['data']._versions:\n",
    "        if vname != '__first_version__':\n",
    "            version = test['data'][vname]\n",
    "            group_key = list(version.keys())[0]\n",
    "            lengths.append(len(version[group_key]['val']))\n",
    "            total_size += len(version[group_key]['val'])\n",
    "    test['theoretical_sizes'] = 24*total_size\n",
    "    print(f\"Maximum array size for file with {test['num_transactions']} transactions: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note also that changing the chunking parameter does not change the size of the arrays, so we read the file with chunksize $2^{14}$ since we have observed faster read times for this file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger values we have something like this:\n",
    "```\n",
    "Maximum array size for file with 50 transactions: 5500\n",
    "Maximum array size for file with 100 transactions: 6000\n",
    "Maximum array size for file with 500 transactions: 10000\n",
    "Maximum array size for file with 1000 transactions: 15000\n",
    "Maximum array size for file with 5000 transactions: 55000\n",
    "Maximum array size for file with 10000 transactions: 105000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sizes_1 = [test['theoretical_sizes'] for test in tests_complete[-n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some irrelevant info from the dictionary \n",
    "test_large_fraction_changes_sparse = []\n",
    "for test in tests_complete:\n",
    "    test_large_fraction_changes_sparse.append(dict((k, test[k]) for k in ['num_transactions', 'filename', 'size', 'size_label', 't_write']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the size information in a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filesizes_1 = np.array([test['size'] for test in test_large_fraction_changes_sparse])\n",
    "sizelabels_1 = np.array([test['size_label'] for test in test_large_fraction_changes_sparse])\n",
    "\n",
    "fig_large_fraction_changes = plt.figure(figsize=(14,10))\n",
    "plt.plot(num_transactions_1, t_sizes_1, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_1, filesizes_1[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_large_fraction_changes_sparse\")\n",
    "plt.legend()\n",
    "labels = [0, 2, 3, 4, 5, 8, 17]\n",
    "plt.yticks(filesizes_1[labels], sizelabels_1[labels])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the view to a logarithmic scale, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_large_fraction_changes_log = plt.figure(figsize=(14,10))\n",
    "plt.loglog(num_transactions_1, t_sizes_1, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.loglog(num_transactions_1, filesizes_1[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_large_fraction_changes_sparse\")\n",
    "plt.legend()\n",
    "labels = [0, 2, 3, 4, 5, 8, 17]\n",
    "plt.yticks(filesizes_1[labels], sizelabels_1[labels])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_write_1 = np.array([test['t_write'] for test in test_large_fraction_changes_sparse])\n",
    "\n",
    "fig_large_fraction_changes_times = plt.figure()\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_1, t_write_1[j*n+i*nexp*n:(j+1)*n+i*nexp*n], 'o--', ms=8, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_large_fraction_changes_sparse - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can clearly see that files with smallest file size, corresponding to chunk sizes of $2^8$ and $2^{10}$, are also the ones with largest creation times. **This is consistent with the effects of using smaller chunk sizes in HDF5 files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously executed tests (with no compression), with larger numbers of transactions, gave us the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_large_fraction_changes_sparse_versions_8.pickle\", \"rb\") as pickle_in:\n",
    "    test_large_fraction_changes_sparse_8 = pickle.load(pickle_in)\n",
    "with open(\"test_large_fraction_changes_sparse_versions_10.pickle\", \"rb\") as pickle_in:\n",
    "    test_large_fraction_changes_sparse_10 = pickle.load(pickle_in)\n",
    "with open(\"test_large_fraction_changes_sparse_versions_12.pickle\", \"rb\") as pickle_in:\n",
    "    test_large_fraction_changes_sparse_12 = pickle.load(pickle_in)\n",
    "with open(\"test_large_fraction_changes_sparse_versions_14.pickle\", \"rb\") as pickle_in:\n",
    "    test_large_fraction_changes_sparse_14 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_1 = [50, 100, 500, 1000, 5000, 10000]\n",
    "# Previously computed theoretical sizes\n",
    "t_sizes_1 = [6426000, 13332000, 90180000, 240240000, 3600720000, 13201320000]\n",
    "\n",
    "filesizes_1_8 = np.array([test['size'] for test in test_large_fraction_changes_sparse_8])\n",
    "sizelabels_1_8 = np.array([test['size_label'] for test in test_large_fraction_changes_sparse_8])\n",
    "\n",
    "filesizes_1_10 = np.array([test['size'] for test in test_large_fraction_changes_sparse_10])\n",
    "sizelabels_1_10 = np.array([test['size_label'] for test in test_large_fraction_changes_sparse_10])\n",
    "\n",
    "filesizes_1_12 = np.array([test['size'] for test in test_large_fraction_changes_sparse_12])\n",
    "sizelabels_1_12 = np.array([test['size_label'] for test in test_large_fraction_changes_sparse_12])\n",
    "\n",
    "filesizes_1_14 = np.array([test['size'] for test in test_large_fraction_changes_sparse_14])\n",
    "sizelabels_1_14 = np.array([test['size_label'] for test in test_large_fraction_changes_sparse_14])\n",
    "\n",
    "fig_large_fraction_changes_pickled = plt.figure(figsize=(14,10))\n",
    "plt.plot(num_transactions_1, filesizes_1_8, '*--', color='blue', ms=12, label=\"Chunk size 2**8\")\n",
    "plt.plot(num_transactions_1, filesizes_1_10, '*--', color='orange', ms=12, label=\"Chunk size 2**10\")\n",
    "plt.plot(num_transactions_1, filesizes_1_12, '*--', color='green', ms=12, label=\"Chunk size 2**12\")\n",
    "plt.plot(num_transactions_1, filesizes_1_14, '*--', color='red', ms=12, label=\"Chunk size 2**14\")\n",
    "plt.plot(num_transactions_1, t_sizes_1, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_large_fraction_changes_sparse - large tests\")\n",
    "plt.legend()\n",
    "labels = [0, 3, 4, 5]\n",
    "plt.yticks(filesizes_1_14[labels], sizelabels_1_14[labels])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a log plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14,10), sharey=True)\n",
    "\n",
    "ax1.loglog(num_transactions_1, t_sizes_1, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax1.loglog(num_transactions_1, filesizes_1_8, '*-', ms=12, color='blue', label=\"Chunk size 2**8\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.loglog(num_transactions_1, t_sizes_1, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax2.loglog(num_transactions_1, filesizes_1_10, '*-', ms=12, color='orange', label=\"Chunk size 2**10\")\n",
    "ax2.legend()\n",
    "\n",
    "ax3.loglog(num_transactions_1, t_sizes_1, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax3.loglog(num_transactions_1, filesizes_1_12, '*-', ms=12, color='green', label=\"Chunk size 2**12\")\n",
    "ax3.legend()\n",
    "\n",
    "ax4.loglog(num_transactions_1, t_sizes_1, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax4.loglog(num_transactions_1, filesizes_1_14, '*-', ms=12, color='red', label=\"Chunk size 2**14\")\n",
    "ax4.legend()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "f.suptitle(\"test_large_fraction_changes_sparse - large tests\")\n",
    "\n",
    "plt.yticks(filesizes_1_14, sizelabels_1_14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behaviour suggests that for `test_large_fraction_changes_sparse`, larger chunk sizes generate larger files, but the size of the files grows modestly as the number of transactions grow. So, **if we are dealing with a large number of transactions, larger chunk sizes generate files that are of reasonable size while having faster creation times** (and probably faster IO speeds as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests_complete:\n",
    "    test['h5pyfile'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test2'></a>\n",
    "\n",
    "# Test 2: Mostly appends (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_mostly_appends_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we have tested the following number of transactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_2 = [50, 100, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we tested chunk sizes of $2^8$, $2^{10}$, $2^{12}$ and $2^{14}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_2` and `exponents` as desired (the previous warning applies: beware of very large file sizes and creation times for large numbers of versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_2 = [50, 100]\n",
    "#exponents = [8, 10, 12, 14]\n",
    "exponents = [12, 14]\n",
    "compression = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files and set up dictionary with test info.\n",
    "tests_complete = []\n",
    "for c in compression:\n",
    "    for p in exponents:\n",
    "        for n in num_transactions_2:\n",
    "            chunk_size = 2**p\n",
    "            name = f\"{testname}_{n}_{p}_{c}\"\n",
    "            filename = os.path.join(path, f\"{name}.h5\")\n",
    "            print(\"File with\\n\" \\\n",
    "                  f\"- {n} transactions\\n\" \\\n",
    "                  f\"- chunk size 2**{p}\\n\"\\\n",
    "                  f\"- compression filter {c}\")\n",
    "            try:\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "                print(\"already exists - unable to compute creation time.\")\n",
    "                t = 0\n",
    "            except:\n",
    "                print(\"not available. Creating new file.\")\n",
    "                t0 = time.time()\n",
    "                TestVersionedDatasetPerformance().test_mostly_appends_sparse(n, name, chunk_size, c)\n",
    "                t = time.time()-t0\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "            data = VersionedHDF5File(h5pyfile)\n",
    "            tests_complete.append(dict(num_transactions=n, \n",
    "                                       chunk_size=chunk_size, \n",
    "                                       filename=filename, \n",
    "                                       h5pyfile=h5pyfile, \n",
    "                                       data=data, \n",
    "                                       t_write=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute sizes\n",
    "for test in tests_complete:\n",
    "    test['size'] = os.path.getsize(test['filename'])\n",
    "    test['size_label'] = format_size(test['size'])\n",
    "    \n",
    "n = len(num_transactions_2)\n",
    "# We take just the last n items in test_complete because usually \n",
    "# they will be the fastest to read/write \n",
    "for test in tests_complete[-n:]:\n",
    "    lengths = []\n",
    "    total_size = 0\n",
    "    for vname in test['data']._versions:\n",
    "        if vname != '__first_version__':\n",
    "            version = test['data'][vname]\n",
    "            group_key = list(version.keys())[0]\n",
    "            lengths.append(len(version[group_key]['val']))\n",
    "            total_size += len(version[group_key]['val'])\n",
    "    test['theoretical_sizes'] = 24*total_size\n",
    "    print(f\"Maximum array size for file with {test['num_transactions']}: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sizes_2 = [test['theoretical_sizes'] for test in tests_complete[-n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing irrelevant information from the test dictionary\n",
    "test_mostly_appends_sparse = []\n",
    "for test in tests_complete:\n",
    "    test_mostly_appends_sparse.append(dict((k, test[k]) for k in ['num_transactions', 'filename', 'size', 'size_label', 't_write']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesizes_2 = np.array([test['size'] for test in test_mostly_appends_sparse])\n",
    "sizelabels_2 = np.array([test['size_label'] for test in test_mostly_appends_sparse])\n",
    "\n",
    "fig_mostly_appends = plt.figure(figsize=(14,10))\n",
    "plt.plot(num_transactions_2, t_sizes_2, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_2, filesizes_2[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "    \n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_mostly_appends_sparse\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_2[-n:], sizelabels_2[-n:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the view to a logarithmic scale, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mostly_appends_log = plt.figure(figsize=(14,10))\n",
    "plt.loglog(num_transactions_2, t_sizes_2, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.loglog(num_transactions_2, filesizes_2[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_mostly_appends_sparse\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_2[-n:], sizelabels_2[-n:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_write_2 = np.array([test['t_write'] for test in test_mostly_appends_sparse])\n",
    "\n",
    "fig_mostly_appends_times = plt.figure()\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_2, t_write_2[j*n+i*nexp*n:(j+1)*n+i*nexp*n], 'o--', ms=8, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_mostly_appends_sparse - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar to the first test: smaller chunk sizes correspond to smaller file sizes, but larger creation times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests_complete:\n",
    "    test['h5pyfile'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test3'></a>\n",
    "\n",
    "# Test 3: Small fraction changes (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_small_fraction_changes_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tested the following numbers of versions (or transactions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_3 = [50, 100, 500, 1000, 5000]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_3` and `exponents` as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_3 = [50, 100, 500]\n",
    "#exponents = [8, 10, 12, 14]\n",
    "exponents = [12, 14]\n",
    "compression = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files and set up dictionary with test info.\n",
    "tests_complete = []\n",
    "for c in compression:\n",
    "    for p in exponents:\n",
    "        for n in num_transactions_3:\n",
    "            chunk_size = 2**p\n",
    "            name = f\"{testname}_{n}_{p}_{c}\"\n",
    "            filename = os.path.join(path, f\"{name}.h5\")\n",
    "            print(\"File with\\n\" \\\n",
    "                  f\"- {n} transactions\\n\" \\\n",
    "                  f\"- chunk size 2**{p}\\n\"\\\n",
    "                  f\"- compression filter {c}\")\n",
    "            try:\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "                print(\"already exists - unable to compute creation time.\")\n",
    "                t = 0\n",
    "            except:\n",
    "                print(\"not available. Creating new file.\")\n",
    "                t0 = time.time()\n",
    "                TestVersionedDatasetPerformance().test_small_fraction_changes_sparse(n, name, chunk_size, c)\n",
    "                t = time.time()-t0\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "            data = VersionedHDF5File(h5pyfile)\n",
    "            tests_complete.append(dict(num_transactions=n, \n",
    "                                       chunk_size=chunk_size, \n",
    "                                       filename=filename, \n",
    "                                       h5pyfile=h5pyfile, \n",
    "                                       data=data, \n",
    "                                       t_write=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute sizes\n",
    "for test in tests_complete:\n",
    "    test['size'] = os.path.getsize(test['filename'])\n",
    "    test['size_label'] = format_size(test['size'])\n",
    "    \n",
    "n = len(num_transactions_3)\n",
    "for test in tests_complete[-n:]:\n",
    "    lengths = []\n",
    "    total_size = 0\n",
    "    for vname in test['data']._versions:\n",
    "        if vname != '__first_version__':\n",
    "            version = test['data'][vname]\n",
    "            group_key = list(version.keys())[0]\n",
    "            lengths.append(len(version[group_key]['val']))\n",
    "            total_size += len(version[group_key]['val'])\n",
    "    test['theoretical_sizes'] = 24*total_size\n",
    "    print(f\"Maximum array size for file with {test['num_transactions']}: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that changing the chunking parameter does not change the size of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sizes_3 = [test['theoretical_sizes'] for test in tests_complete[-n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing irrelevant information\n",
    "test_small_fraction_changes_sparse = []\n",
    "for test in tests_complete:\n",
    "    test_small_fraction_changes_sparse.append(dict((k, test[k]) for k in ['num_transactions', 'filename', 'size', 'size_label', 't_write']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filesizes_3 = np.array([test['size'] for test in test_small_fraction_changes_sparse])\n",
    "sizelabels_3 = np.array([test['size_label'] for test in test_small_fraction_changes_sparse])\n",
    "\n",
    "fig_small_fraction_changes = plt.figure(figsize=(14,10))\n",
    "plt.plot(num_transactions_3, t_sizes_3, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_3, filesizes_3[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_small_fraction_changes_sparse\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_3[-n:], sizelabels_3[-n:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the view to a logarithmic scale, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_small_fraction_changes_log = plt.figure(figsize=(14,10))\n",
    "plt.loglog(num_transactions_3, t_sizes_3, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.loglog(num_transactions_3, filesizes_3[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_small_fraction_changes_sparse\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_3[-n:], sizelabels_3[-n:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_write_3 = np.array([test['t_write'] for test in test_small_fraction_changes_sparse])\n",
    "\n",
    "fig_small_fraction_changes_times = plt.figure()\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_3, t_write_3[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_small_fraction_changes_sparse - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can clearly see that the files with smallest file size, corresponding to chunk sizes of $2^8$ and $2^{10}$, are also the ones with largest creation times. This is consistent with the effects of using smaller chunk sizes in HDF5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously executed tests (with no compression), with larger numbers of transactions, gave us the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_small_fraction_changes_sparse_versions_8.pickle\", \"rb\") as pickle_in:\n",
    "    test_small_fraction_changes_sparse_8 = pickle.load(pickle_in)\n",
    "with open(\"test_small_fraction_changes_sparse_versions_10.pickle\", \"rb\") as pickle_in:\n",
    "    test_small_fraction_changes_sparse_10 = pickle.load(pickle_in)\n",
    "with open(\"test_small_fraction_changes_sparse_versions_12.pickle\", \"rb\") as pickle_in:\n",
    "    test_small_fraction_changes_sparse_12 = pickle.load(pickle_in)\n",
    "with open(\"test_small_fraction_changes_sparse_versions_14.pickle\", \"rb\") as pickle_in:\n",
    "    test_small_fraction_changes_sparse_14 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_3 = [50, 100, 500, 1000, 5000]\n",
    "# Previously computed theoretical sizes\n",
    "t_sizes_3 = [6426000, 13332000, 90180000, 240240000, 3600720000]\n",
    "\n",
    "filesizes_3_8 = np.array([test['size'] for test in test_small_fraction_changes_sparse_8])\n",
    "sizelabels_3_8 = np.array([test['size_label'] for test in test_small_fraction_changes_sparse_8])\n",
    "\n",
    "filesizes_3_10 = np.array([test['size'] for test in test_small_fraction_changes_sparse_10])\n",
    "sizelabels_3_10 = np.array([test['size_label'] for test in test_small_fraction_changes_sparse_10])\n",
    "\n",
    "filesizes_3_12 = np.array([test['size'] for test in test_small_fraction_changes_sparse_12])\n",
    "sizelabels_3_12 = np.array([test['size_label'] for test in test_small_fraction_changes_sparse_12])\n",
    "\n",
    "filesizes_3_14 = np.array([test['size'] for test in test_small_fraction_changes_sparse_14])\n",
    "sizelabels_3_14 = np.array([test['size_label'] for test in test_small_fraction_changes_sparse_14])\n",
    "\n",
    "fig_large_fraction_changes_pickled = plt.figure(figsize=(14,10))\n",
    "plt.plot(num_transactions_3, filesizes_3_8, '*--', color='blue', ms=12, label=\"Chunk size 2**8\")\n",
    "plt.plot(num_transactions_3, filesizes_3_10, '*--', color='orange', ms=12, label=\"Chunk size 2**10\")\n",
    "plt.plot(num_transactions_3, filesizes_3_12, '*--', color='green', ms=12, label=\"Chunk size 2**12\")\n",
    "plt.plot(num_transactions_3, filesizes_3_14, '*--', color='red', ms=12, label=\"Chunk size 2**14\")\n",
    "plt.plot(num_transactions_3, t_sizes_3, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_small_fraction_changes_sparse - large tests\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_3_14[[0, 3, 4]], sizelabels_3_14[[0, 3, 4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a log plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14,10), sharey=True)\n",
    "\n",
    "ax1.loglog(num_transactions_3, t_sizes_3, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax1.loglog(num_transactions_3, filesizes_3_8, '*-', ms=12, color='blue', label=\"Chunk size 2**8\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.loglog(num_transactions_3, t_sizes_3, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax2.loglog(num_transactions_3, filesizes_3_10, '*-', ms=12, color='orange', label=\"Chunk size 2**10\")\n",
    "ax2.legend()\n",
    "\n",
    "ax3.loglog(num_transactions_3, t_sizes_3, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax3.loglog(num_transactions_3, filesizes_3_12, '*-', ms=12, color='green', label=\"Chunk size 2**12\")\n",
    "ax3.legend()\n",
    "\n",
    "ax4.loglog(num_transactions_3, t_sizes_3, 'o-', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "ax4.loglog(num_transactions_3, filesizes_3_14, '*-', ms=12, color='red', label=\"Chunk size 2**14\")\n",
    "ax4.legend()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "f2.suptitle(\"test_small_fraction_changes_sparse - large tests\")\n",
    "\n",
    "plt.yticks(filesizes_3_14, sizelabels_3_14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behaviour is similar to what we got in the `test_large_fraction_changes_sparse` case: for `test_small_fraction_changes_sparse`, larger chunk sizes generate larger files, but the size of the files grows modestly as the number of transactions grow. So, **if we are dealing with a large number of transactions, larger chunk sizes generate files that are of reasonable size while having faster creation times** (and probably faster IO speeds as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests_complete:\n",
    "    test['h5pyfile'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test4'></a>\n",
    "\n",
    "# Test 4: Mostly appends (dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_mostly_appends_dense\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we have tested the following number of transactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_2 = [50, 100, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_4` and `exponents` as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_4 = [50, 100]\n",
    "#exponents = [8, 10, 12, 14]\n",
    "exponents = [12, 14]\n",
    "compression = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files and set up dictionary with test info.\n",
    "tests_complete = []\n",
    "for c in compression:\n",
    "    for p in exponents:\n",
    "        for n in num_transactions_4:\n",
    "            chunk_size = 2**p\n",
    "            name = f\"{testname}_{n}_{p}_{c}\"\n",
    "            filename = os.path.join(path, f\"{name}.h5\")\n",
    "            print(\"File with\\n\" \\\n",
    "                  f\"- {n} transactions\\n\" \\\n",
    "                  f\"- chunk size 2**{p}\\n\"\\\n",
    "                  f\"- compression filter {c}\")\n",
    "            try:\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "                print(\"already exists - unable to compute creation time.\")\n",
    "                t = 0\n",
    "            except:\n",
    "                print(\"not available. Creating new file.\")\n",
    "                t0 = time.time()\n",
    "                TestVersionedDatasetPerformance().test_mostly_appends_dense(n, name, chunk_size, c)\n",
    "                t = time.time()-t0\n",
    "                h5pyfile = h5py.File(filename, 'r')\n",
    "            data = VersionedHDF5File(h5pyfile)\n",
    "            tests_complete.append(dict(num_transactions=n, \n",
    "                                       chunk_size=chunk_size, \n",
    "                                       filename=filename, \n",
    "                                       h5pyfile=h5pyfile, \n",
    "                                       data=data, \n",
    "                                       t_write=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests_complete:\n",
    "    test['size'] = os.path.getsize(test['filename'])\n",
    "    test['size_label'] = format_size(test['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = len(num_transactions_4)\n",
    "for test in tests_complete[-n:]:\n",
    "    lengths = []\n",
    "    total_size = 0\n",
    "    for vname in test['data']._versions:\n",
    "        if vname != '__first_version__':\n",
    "            version = test['data'][vname]\n",
    "            group_key = list(version.keys())[0]\n",
    "            lengths.append(len(version[group_key]['val']))\n",
    "            total_size += len(version[group_key]['val'])\n",
    "    test['theoretical_sizes'] = 24*total_size\n",
    "    print(f\"Maximum array size for file with {test['num_transactions']} transactions: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that changing the chunking parameter does not change the size of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sizes_4 = [test['theoretical_sizes'] for test in tests_complete[-n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mostly_appends_dense = []\n",
    "for test in tests_complete:\n",
    "    test_mostly_appends_dense.append(dict((k, test[k]) for k in ['num_transactions', 'filename', 'size', 'size_label', 't_write']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filesizes_4 = np.array([test['size'] for test in test_mostly_appends_dense])\n",
    "sizelabels_4 = np.array([test['size_label'] for test in test_mostly_appends_dense])\n",
    "\n",
    "fig_mostly_appends_dense = plt.figure(figsize=(14,10))\n",
    "plt.plot(num_transactions_4, t_sizes_4, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_4, filesizes_4[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_mostly_appends_dense\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_4[-n:], sizelabels_4[-n:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the view to a logarithmic scale, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mostly_appends_dense_log = plt.figure(figsize=(14,10))\n",
    "\n",
    "plt.loglog(num_transactions_4, t_sizes_4, 'o--', ms=5, color='magenta', label=\"Theoretical file size\")\n",
    "\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.loglog(num_transactions_4, filesizes_4[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_mostly_appends_dense\")\n",
    "plt.legend()\n",
    "plt.yticks(filesizes_4[-n:], sizelabels_4[-n:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_write_4 = np.array([test['t_write'] for test in test_mostly_appends_dense])\n",
    "\n",
    "fig_mostly_appends_dense_times = plt.figure()\n",
    "nexp = len(exponents)\n",
    "for i in range(len(compression)):\n",
    "    for j in range(nexp):\n",
    "        plt.plot(num_transactions_4, filesizes_4[j*n+i*nexp*n:(j+1)*n+i*nexp*n], '*--', ms=12, label=f\"Chunk size 2**{exponents[j]}, {compression[i]}\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(\"test_mostly_appends_dense - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions_4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behaviour is similar to what we observed in other tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests_complete:\n",
    "    test['h5pyfile'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each versioned HDF5 file contains 3 datasets per version:\n",
    "- `key0`, an array of `int64`\n",
    "- `key1`, an array of `int64`\n",
    "- `val`, an array of `float64`\n",
    "plus metadata about groups, datasets and versions.\n",
    "\n",
    "This means that each file has  \n",
    "\n",
    "```\n",
    "nversions * 24 * arraysize + metadata\n",
    "```\n",
    "bytes of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standard'></a>\n",
    "## Standard parameters\n",
    "\n",
    "- `test_large_fraction_changes_sparse`: \n",
    "    - `num_rows_initial = 5000`\n",
    "    - `num_rows_per_append = 10`\n",
    "    - `num_inserts = 10`\n",
    "    - `num_deletes = 10`\n",
    "    - `num_changes = 1000`\n",
    "- `test_small_fraction_changes_sparse`\n",
    "    - `num_rows_initial = 5000`\n",
    "    - `num_rows_per_append = 10`\n",
    "    - `num_inserts = 10`\n",
    "    - `num_deletes = 10`\n",
    "    - `num_changes = 10`\n",
    "- `test_mostly_appends_sparse`:\n",
    "    - `num_rows_initial = 1000`\n",
    "    - `num_rows_per_append = 1000`\n",
    "    - `num_inserts = 10`\n",
    "    - `num_deletes = 10`\n",
    "    - `num_changes = 10`  \n",
    "- `test_mostly_appends_dense`\n",
    "    - `num_rows_initial_0 = 30`\n",
    "    - `num_rows_initial_1 = 30`\n",
    "    - `num_rows_per_append_0 = 1`\n",
    "    - `num_inserts_0 = 1`\n",
    "    - `num_inserts_1 = 10`\n",
    "    - `num_deletes_0 = 1`\n",
    "    - `num_deletes_1 = 1`\n",
    "    - `num_changes = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
