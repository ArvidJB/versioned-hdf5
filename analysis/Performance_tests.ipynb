{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='home'></a>\n",
    "\n",
    "# Performance Analysis of VersionedHDF5 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these tests, we have generated `.h5` data files using the `generate_data_deterministic.py` script from the [VersionedHDF5 repository](https://github.com/deshaw/versioned-hdf5), using the standard options ([see details here](#standard))\n",
    "\n",
    "We performed the following tests:\n",
    "1. [Test Large Fraction Changes Sparse](#test1)\n",
    "2. [Test Mostly Appends Sparse](#test2)\n",
    "3. [Test Small Fraction Changes Sparse](#test3)\n",
    "4. [Test Mostly Appends Dense](#test4)\n",
    "5. [Test Large Fraction Changes (Constant Array Size) Sparse](#test5)\n",
    "\n",
    "**These tests were last run on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.utcnow(), \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the generated test files is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/melissa/projects/versioned-hdf5/analysis\" # change this as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import performance_tests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information from the generated test files are stored in either\n",
    "- `testcase.tests`, a dictionary containing all the info related to a testcase that was run recently;\n",
    "- a `.json` file named after the test name and options, containing a summary of the results. This file can be read with\n",
    "    ```python\n",
    "    with open(\"<filename>.json\", \"r\") as json_in:\n",
    "        test = json.load(json_in)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test1'></a>\n",
    "\n",
    "# Test 1: Large fraction changes (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_large_fraction_changes_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tested the following numbers of versions (or transactions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_1 = [50, 100, 500, 1000, 5000, 10000]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to generate the files now, modify the following constants for the desired tests. **Please keep in mind that file sizes can become very large for large numbers of transactions (above 5000 transactions).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_1 = [50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chunk size parameter, we have tested chunk sizes of $2^8, 2^{10}, 2^{12}$ and $2^{14}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponents_1 = [12, 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose desired compression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_1 = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = performance_tests.test_large_fraction_changes_sparse(path=path,\n",
    "                                                                num_transactions=num_transactions_1, \n",
    "                                                                exponents=exponents_1, \n",
    "                                                                compression=compression_1)\n",
    "testcase_1, msg = testcase.create_files()\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison, if you wish to generate the same tests with **no versioning**, meaning no use of the `VersionedHDF5` library, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_1_no_versions, msg = testcase.create_files(versions=False)\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the results for later, execute the cell below. **CAUTION: This may erase previous versions of saved tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcase.save(testcase_1, f\"{testname}\")\n",
    "#testcase.save(testcase_1_no_versions, f\"{testname}_no_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading previously computed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, you can read an existing `.json` file by uncommenting the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{testname}.json\", \"r\") as json_in:\n",
    "    testcase_1 = json.load(json_in)\n",
    "with open(f\"{testname}_no_versions.json\", \"r\") as json_in:\n",
    "    testcase_1_no_versions = json.load(json_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's obtain some common parameters from the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions = [test['num_transactions'] for test in testcase_1]\n",
    "chunk_sizes = [test['chunk_size'] for test in testcase_1]\n",
    "compression = [test['compression'] for test in testcase_1]\n",
    "filesizes = np.array([test['size'] for test in testcase_1])\n",
    "sizelabels = np.array([test['size_label'] for test in testcase_1])\n",
    "\n",
    "n = len(set(num_transactions))\n",
    "ncs = len(set(chunk_sizes))\n",
    "ncomp = len(set(compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by analyzing how the `.h5` file sizes grow as the number of versions grows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the array size also grows as the number of versions grows, since each transaction is changing the original arrays by adding, deleting and changing values in the original arrays. In order to compute a (naive) theoretical lower bound on the file size, we'll compute how much space each version should take. Keep in mind there is redundant data as some of it is not changed during the staging of a new version but it is still being stored. In this example, we start with three arrays with 5000 elements (2 integer arrays and one float), and in the end we have the following array sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Maximum array size for file with 50 transactions: 5500\n",
    "Maximum array size for file with 100 transactions: 6000\n",
    "Maximum array size for file with 500 transactions: 10000\n",
    "Maximum array size for file with 1000 transactions: 15000\n",
    "Maximum array size for file with 5000 transactions: 55000\n",
    "Maximum array size for file with 10000 transactions: 105000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the size information in a plot. On the left, we can see a linear plot, and on the right a loglog plot of the same size data for `testcase_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "\n",
    "# Changing the indices in selected will change the y-axis ticks in the graph for better visualization\n",
    "#selected = (range(len(filesizes)))\n",
    "selected = [0, 4, 5, 10, 11]\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[0].plot(num_transactions[:n],\n",
    "                   filesizes[start+j*n:start+(j+1)*n],\n",
    "                   '*--', ms=12, \n",
    "                   label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "\n",
    "        ax[1].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[0].legend(loc='upper left')\n",
    "        ax[1].legend(loc='upper left')\n",
    "        ax[0].minorticks_off()\n",
    "        ax[1].minorticks_off()\n",
    "        ax[0].set_xticks(num_transactions[:n])\n",
    "        ax[0].set_xticklabels(num_transactions[:n])\n",
    "        ax[0].set_yticks(filesizes[selected])\n",
    "        ax[0].set_yticklabels(sizelabels[selected])\n",
    "        ax[0].set_xlabel(\"Transactions\")\n",
    "        ax[0].grid(True)\n",
    "        ax[1].set_xticks(num_transactions[:n])\n",
    "        ax[1].set_xticklabels(num_transactions[:n])\n",
    "        ax[1].set_yticks(filesizes[selected])\n",
    "        ax[1].set_yticklabels(sizelabels[selected])\n",
    "        ax[1].set_xlabel(\"Transactions\")\n",
    "        ax[1].grid(True)\n",
    "\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing compression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chunk size that we chose to test, let's compare the file sizes corresponding to each compression algorithm that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncs, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "\n",
    "selected = [0, 2, 4, 5]\n",
    "    \n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[j].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=f\"{compression[start]}\")\n",
    "        ax[j].legend(loc='upper left')\n",
    "        ax[j].set_title(f\"Chunk Size {chunk_sizes[start+j*n]}\")\n",
    "        ax[j].set_xticks(num_transactions[:n])\n",
    "        ax[j].set_xticklabels(num_transactions[:n])\n",
    "        ax[j].set_yticks(filesizes[selected])\n",
    "        ax[j].set_yticklabels(sizelabels[selected])\n",
    "        ax[j].grid(True)\n",
    "        ax[j].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each choice of compression algorithm, we compare different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncomp, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "    \n",
    "selected = [0, 5, 6]\n",
    "    \n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plotlabel = f\"Chunk size {chunk_sizes[start+j*n]}\"\n",
    "        plottitle = f\"Compression: {compression[start]}\"\n",
    "        ax[i].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=plotlabel)\n",
    "        ax[i].legend(loc='upper left')\n",
    "        ax[i].set_title(plottitle)\n",
    "        ax[i].set_xticks(num_transactions[:n])\n",
    "        ax[i].set_xticklabels(num_transactions[:n])\n",
    "        ax[i].set_yticks(filesizes[selected])\n",
    "        ax[i].set_yticklabels(sizelabels[selected])\n",
    "        ax[i].grid(False)\n",
    "        ax[i].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_write = np.array([test['t_write'][-1] for test in testcase_1])\n",
    "fig_large_fraction_changes_times = plt.figure(figsize=(10,8))\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write[start+j*n:start+(j+1)*n], \n",
    "                 'o--', ms=8, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "t_write_nv = np.array([test['t_write'][-1] for test in testcase_1_no_versions])\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write_nv[start+j*n:start+(j+1)*n], \n",
    "                 '*-', ms=12, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}, No versioning\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(f\"{testname} - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions[:n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at the time required to stage a new version in the file, that is, to add a new transaction. The graphs below show, for each fixed number of transactions, the time required to add new versions as the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_times, ax = plt.subplots(n+1, figsize=(14,20))\n",
    "fig_times.suptitle(f\"{testname}: time to write each new version\")\n",
    "\n",
    "for i in range(n):\n",
    "    for test in testcase_1:\n",
    "        if test['num_transactions'] == num_transactions[i]:\n",
    "            t_write = np.array(test['t_write'][:-1])\n",
    "            ax[i].plot(t_write, \n",
    "                       label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "            ax[i].legend(loc='upper left')\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "for test in testcase_1_no_versions:\n",
    "    if test['num_transactions'] == num_transactions[i]:\n",
    "        t_write = np.array(test['t_write'][:-1])\n",
    "        ax[n].plot(t_write, \n",
    "                   label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "        ax[n].legend(loc='upper left')\n",
    "        ax[n].set_title('No versioning')\n",
    "            \n",
    "plt.xlabel(\"Number of transactions\")\n",
    "plt.ylabel(\"Time (in seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that files with smallest file size, corresponding to smaller chunk sizes, are also the ones with largest creation times. **This is consistent with the effects of using smaller chunk sizes in HDF5 files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behaviour suggests that for `test_large_fraction_changes_sparse`, larger chunk sizes generate larger files, but the size of the files grows modestly as the number of transactions grow. So, **if we are dealing with a large number of transactions, larger chunk sizes generate files that are of reasonable size while having faster creation times** (and probably faster IO speeds as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test2'></a>\n",
    "\n",
    "# Test 2: Mostly appends (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_mostly_appends_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we have tested the following number of transactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_2 = [50, 100, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_2`, `exponents_2` and `compression_2` as desired (the previous warning applies: beware of very large file sizes and creation times for large numbers of versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_2 = [25, 50, 100, 200]\n",
    "exponents_2 = [12, 14]\n",
    "compression_2 = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = performance_tests.test_mostly_appends_sparse(path=path,\n",
    "                                                        num_transactions=num_transactions_2, \n",
    "                                                        exponents=exponents_2, \n",
    "                                                        compression=compression_2)\n",
    "testcase_2, msg = testcase.create_files()\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison, if you wish to generate the same tests with **no versioning**, meaning no use of the `VersionedHDF5` library, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_2_no_versions, msg = testcase.create_files(versions=False)\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the results for later, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcase.save(testcase_2, f\"{testname}\")\n",
    "#testcase.save(testcase_2_no_versions, f\"{testname}_no_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading previously computed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an existing `.json` file, uncomment the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{testname}.json\", \"r\") as json_in:\n",
    "    testcase_2 = json.load(json_in)\n",
    "with open(f\"{testname}_no_versions.json\", \"r\") as json_in:\n",
    "    testcase_2_no_versions = json.load(json_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same analysis as in the previous test, let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions = [test['num_transactions'] for test in testcase_2]\n",
    "chunk_sizes = [test['chunk_size'] for test in testcase_2]\n",
    "compression = [test['compression'] for test in testcase_2]\n",
    "filesizes = np.array([test['size'] for test in testcase_2])\n",
    "sizelabels = np.array([test['size_label'] for test in testcase_2])\n",
    "\n",
    "n = len(set(num_transactions))\n",
    "ncs = len(set(chunk_sizes))\n",
    "ncomp = len(set(compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we did before, on the left we can see a linear plot, and on the right a loglog plot of the same size data for `testcase_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "\n",
    "selected = [0, 7, 10, 11, 23]\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[0].plot(num_transactions[:n],\n",
    "                   filesizes[start+j*n:start+(j+1)*n], \n",
    "                   '*--', ms=12, \n",
    "                   label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[1].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n], \n",
    "                     '*--', ms=12, \n",
    "                     label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[0].legend(loc='upper left')\n",
    "        ax[1].legend(loc='upper left')\n",
    "        ax[0].minorticks_off()\n",
    "        ax[1].minorticks_off()\n",
    "        # Changing the indices in selected will change the y-axis ticks in the graph for better visualization\n",
    "        ax[0].set_xticks(num_transactions[:n])\n",
    "        ax[0].set_xticklabels(num_transactions[:n])\n",
    "        ax[0].set_yticks(filesizes[selected])\n",
    "        ax[0].set_yticklabels(sizelabels[selected])\n",
    "        ax[0].set_xlabel(\"Transactions\")\n",
    "        ax[0].grid(True)\n",
    "        ax[1].set_xticks(num_transactions[:n])\n",
    "        ax[1].set_xticklabels(num_transactions[:n])\n",
    "        ax[1].set_yticks(filesizes[selected])\n",
    "        ax[1].set_yticklabels(sizelabels[selected])\n",
    "        ax[1].set_xlabel(\"Transactions\")\n",
    "        ax[1].grid(True)\n",
    "\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing compression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chunk size that we chose to test, let's compare the file sizes corresponding to each compression algorithm that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncs, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[j].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=f\"{compression[start]}\")\n",
    "        ax[j].legend(loc='upper left')\n",
    "        ax[j].set_title(f\"Chunk Size {chunk_sizes[start+j*n]}\")\n",
    "        ax[j].set_xticks(num_transactions[:n])\n",
    "        ax[j].set_xticklabels(num_transactions[:n])\n",
    "        ax[j].set_yticks(filesizes[selected])\n",
    "        ax[j].set_yticklabels(sizelabels[selected])\n",
    "        ax[j].grid(True)\n",
    "        ax[j].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each choice of compression algorithm, we compare different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncomp, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "    \n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plotlabel = f\"Chunk size {chunk_sizes[start+j*n]}\"\n",
    "        plottitle = f\"Compression: {compression[start]}\"\n",
    "        ax[i].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=plotlabel)\n",
    "        ax[i].legend(loc='upper left')\n",
    "        ax[i].set_title(plottitle)\n",
    "        ax[i].set_xticks(num_transactions[:n])\n",
    "        ax[i].set_xticklabels(num_transactions[:n])\n",
    "        ax[i].set_yticks(filesizes[selected])\n",
    "        ax[i].set_yticklabels(sizelabels[selected])\n",
    "        ax[i].grid(True)\n",
    "        ax[i].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation times for each file are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_write = np.array([test['t_write'][-1] for test in testcase_2])\n",
    "fig_large_fraction_changes_times = plt.figure(figsize=(10,8))\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write[start+j*n:start+(j+1)*n], \n",
    "                 'o--', ms=8, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "t_write_nv = np.array([test['t_write'][-1] for test in testcase_2_no_versions])\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write_nv[start+j*n:start+(j+1)*n], \n",
    "                 '*-', ms=12, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}, No versioning\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(f\"{testname} - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions[:n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the graphs below show, for each fixed number of transactions, the time required to add new versions as the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_times, ax = plt.subplots(n+1, figsize=(14,20))\n",
    "fig_times.suptitle(f\"{testname}: time to write each new version\")\n",
    "\n",
    "for i in range(n):\n",
    "    for test in testcase_2:\n",
    "        if test['num_transactions'] == num_transactions[i]:\n",
    "            t_write = np.array(test['t_write'][:-1])\n",
    "            ax[i].plot(t_write, \n",
    "                       label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "            ax[i].legend(loc='upper left')\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "for test in testcase_2_no_versions:\n",
    "    if test['num_transactions'] == num_transactions[i]:\n",
    "        t_write = np.array(test['t_write'][:-1])\n",
    "        ax[n].plot(t_write, \n",
    "                   label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "        ax[n].legend(loc='upper left')\n",
    "        ax[n].set_title('No versioning')\n",
    "            \n",
    "plt.xlabel(\"Number of transactions\")\n",
    "plt.ylabel(\"Time (in seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar to the first test: smaller chunk sizes correspond to smaller file sizes, but larger creation times. However, in this case, we can see there is a drop in performance when adding new versions as our file grows. This can be seen as an effect of the increase in the data size for each new version (since we are *mostly appending* data with each new version) but **can't be explained by that alone**, as evidenced by the difference in scale between creation sizes for the versioned and non-versioned cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test3'></a>\n",
    "\n",
    "# Test 3: Small fraction changes (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_small_fraction_changes_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tested the following numbers of versions (or transactions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_3 = [50, 100, 500, 1000, 5000, 10000]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_3`, `exponents_3` and `compression_3` as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_3 = [50, 100, 500]\n",
    "exponents_3 = [12, 14]\n",
    "compression_3 = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = performance_tests.test_small_fraction_changes_sparse(path=path,\n",
    "                                                                num_transactions=num_transactions_3, \n",
    "                                                                exponents=exponents_3, \n",
    "                                                                compression=compression_3)\n",
    "\n",
    "testcase_3, msg = testcase.create_files()\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison, if you wish to generate the same tests with **no versioning**, meaning no use of the `VersionedHDF5` library, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_3_no_versions, msg = testcase.create_files(versions=False)\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the results for later, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcase.save(testcase_3, f\"{testname}\")\n",
    "#testcase.save(testcase_3_no_versions, f\"{testname}_no_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading previously computed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an existing `.json` file, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{testname}.json\", \"r\") as json_in:\n",
    "    testcase_3 = json.load(json_in)\n",
    "with open(f\"{testname}_no_versions.json\", \"r\") as json_in:\n",
    "    testcase_3_no_versions = json.load(json_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions = [test['num_transactions'] for test in testcase_3]\n",
    "chunk_sizes = [test['chunk_size'] for test in testcase_3]\n",
    "compression = [test['compression'] for test in testcase_3]\n",
    "filesizes = np.array([test['size'] for test in testcase_3])\n",
    "sizelabels = np.array([test['size_label'] for test in testcase_3])\n",
    "\n",
    "n = len(set(num_transactions))\n",
    "ncs = len(set(chunk_sizes))\n",
    "ncomp = len(set(compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, on the left we can see a linear plot, and on the right a loglog plot of the same size data for `testcase_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[0].plot(num_transactions[:n],\n",
    "                   filesizes[start+j*n:start+(j+1)*n], \n",
    "                   '*--', ms=12, \n",
    "                   label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[1].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n], \n",
    "                     '*--', ms=12, \n",
    "                     label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[0].legend(loc='upper left')\n",
    "        ax[1].legend(loc='upper left')\n",
    "        ax[0].minorticks_off()\n",
    "        ax[1].minorticks_off()\n",
    "        # Changing the indices in selected will change the y-axis ticks in the graph for better visualization\n",
    "        selected = [0, 4, 5, 11]\n",
    "        ax[0].set_xticks(num_transactions[:n])\n",
    "        ax[0].set_xticklabels(num_transactions[:n])\n",
    "        ax[0].set_yticks(filesizes[selected])\n",
    "        ax[0].set_yticklabels(sizelabels[selected])\n",
    "        ax[0].set_xlabel(\"Transactions\")\n",
    "        ax[0].grid(True)\n",
    "        ax[1].set_xticks(num_transactions[:n])\n",
    "        ax[1].set_xticklabels(num_transactions[:n])\n",
    "        ax[1].set_yticks(filesizes[selected])\n",
    "        ax[1].set_yticklabels(sizelabels[selected])\n",
    "        ax[1].set_xlabel(\"Transactions\")\n",
    "        ax[1].grid(True)\n",
    "\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing compression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chunk size that we chose to test, let's compare the file sizes corresponding to each compression algorithm that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncs, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[j].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=f\"{compression[start]}\")\n",
    "        ax[j].legend(loc='upper left')\n",
    "        ax[j].set_title(f\"Chunk Size {chunk_sizes[start+j*n]}\")\n",
    "        ax[j].set_xticks(num_transactions[:n])\n",
    "        ax[j].set_xticklabels(num_transactions[:n])\n",
    "        ax[j].set_yticks(filesizes[selected])\n",
    "        ax[j].set_yticklabels(sizelabels[selected])\n",
    "        ax[j].grid(False)\n",
    "        ax[j].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each choice of compression algorithm, we compare different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncomp, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "    \n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plotlabel = f\"Chunk size {chunk_sizes[start+j*n]}\"\n",
    "        plottitle = f\"Compression: {compression[start]}\"\n",
    "        ax[i].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=plotlabel)\n",
    "        ax[i].legend(loc='upper left')\n",
    "        ax[i].set_title(plottitle)\n",
    "        ax[i].set_xticks(num_transactions[:n])\n",
    "        ax[i].set_xticklabels(num_transactions[:n])\n",
    "        ax[i].set_yticks(filesizes[selected])\n",
    "        ax[i].set_yticklabels(sizelabels[selected])\n",
    "        ax[i].grid(False)\n",
    "        ax[i].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_write = np.array([test['t_write'][-1] for test in testcase_3])\n",
    "fig_large_fraction_changes_times = plt.figure(figsize=(10,8))\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write[start+j*n:start+(j+1)*n], \n",
    "                 'o--', ms=8, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "t_write_nv = np.array([test['t_write'][-1] for test in testcase_3_no_versions])\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write_nv[start+j*n:start+(j+1)*n], \n",
    "                 '*-', ms=12, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}, No versioning\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(f\"{testname} - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions[:n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the graphs below show, for each fixed number of transactions, the time required to add new versions as the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_times, ax = plt.subplots(n+1, figsize=(14,20))\n",
    "fig_times.suptitle(f\"{testname}: time to write each new version\")\n",
    "\n",
    "for i in range(n):\n",
    "    for test in testcase_3:\n",
    "        if test['num_transactions'] == num_transactions[i]:\n",
    "            t_write = np.array(test['t_write'][:-1])\n",
    "            ax[i].plot(t_write, \n",
    "                       label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "            ax[i].legend(loc='upper left')\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "for test in testcase_3_no_versions:\n",
    "    if test['num_transactions'] == num_transactions[i]:\n",
    "        t_write = np.array(test['t_write'][:-1])\n",
    "        ax[n].plot(t_write, \n",
    "                   label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "        ax[n].legend(loc='upper left')\n",
    "        ax[n].set_title('No versioning')\n",
    "            \n",
    "plt.xlabel(\"Number of transactions\")\n",
    "plt.ylabel(\"Time (in seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behaviour is similar to what we got in the `test_large_fraction_changes_sparse` case, but with larger transaction numbers there is a slight increase in the time required to write new versions as the file size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test4'></a>\n",
    "\n",
    "# Test 4: Mostly appends (dense) - not yet updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_mostly_appends_dense\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we have tested the following number of transactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_4 = [50, 100, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_4`, `exponents_4` and `compression_4` as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_4 = [25, 50, 100, 200]\n",
    "exponents_4 = [12, 14]\n",
    "compression_4 = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = performance_tests.test_mostly_appends_dense(path=path,\n",
    "                                                       num_transactions=num_transactions_4, \n",
    "                                                       exponents=exponents_4, \n",
    "                                                       compression=compression_4)\n",
    "\n",
    "testcase_4, msg = testcase.create_files()\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison, if you wish to generate the same tests with **no versioning**, meaning no use of the `VersionedHDF5` library, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_4_no_versions, msg = testcase.create_files(versions=False)\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the results for later, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcase.save(testcase_4, f\"{testname}\")\n",
    "#testcase.save(testcase_4_no_versions, f\"{testname}_no_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading previously computed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open an existing `.json` file, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f\"{testname}.json\", \"r\") as json_in:\n",
    "#    testcase_4 = json.load(json_in)\n",
    "#with open(f\"{testname}_no_versions.json\", \"r\") as json_in:\n",
    "#    testcase_4_no_versions = json.load(json_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions = [test['num_transactions'] for test in testcase_4]\n",
    "chunk_sizes = [test['chunk_size'] for test in testcase_4]\n",
    "compression = [test['compression'] for test in testcase_4]\n",
    "filesizes = np.array([test['size'] for test in testcase_4])\n",
    "sizelabels = np.array([test['size_label'] for test in testcase_4])\n",
    "\n",
    "n = len(set(num_transactions))\n",
    "ncs = len(set(chunk_sizes))\n",
    "ncomp = len(set(compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, on the left we can see a linear plot, and on the right a loglog plot of the same size data for `testcase_4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "\n",
    "selected = [0, 2, 3, 5, 11]\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[0].plot(num_transactions[:n],\n",
    "                   filesizes[start+j*n:start+(j+1)*n], \n",
    "                   '*--', ms=12, \n",
    "                   label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[1].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n], \n",
    "                     '*--', ms=12, \n",
    "                     label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[0].legend(loc='upper left')\n",
    "        ax[1].legend(loc='upper left')\n",
    "        ax[0].minorticks_off()\n",
    "        ax[1].minorticks_off()\n",
    "        # Changing the indices in selected will change the y-axis ticks in the graph for better visualization\n",
    "        ax[0].set_xticks(num_transactions[:n])\n",
    "        ax[0].set_xticklabels(num_transactions[:n])\n",
    "        ax[0].set_yticks(filesizes[selected])\n",
    "        ax[0].set_yticklabels(sizelabels[selected])\n",
    "        ax[0].set_xlabel(\"Transactions\")\n",
    "        ax[0].grid(True)\n",
    "        ax[1].set_xticks(num_transactions[:n])\n",
    "        ax[1].set_xticklabels(num_transactions[:n])\n",
    "        ax[1].set_yticks(filesizes[selected])\n",
    "        ax[1].set_yticklabels(sizelabels[selected])\n",
    "        ax[1].set_xlabel(\"Transactions\")\n",
    "        ax[1].grid(True)\n",
    "\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing compression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chunk size that we chose to test, let's compare the file sizes corresponding to each compression algorithm that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncs, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[j].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=f\"{compression[start]}\")\n",
    "        ax[j].legend(loc='upper left')\n",
    "        ax[j].set_title(f\"Chunk Size {chunk_sizes[start+j*n]}\")\n",
    "        ax[j].set_xticks(num_transactions[:n])\n",
    "        ax[j].set_xticklabels(num_transactions[:n])\n",
    "        ax[j].set_yticks(filesizes[selected])\n",
    "        ax[j].set_yticklabels(sizelabels[selected])\n",
    "        ax[j].grid(True)\n",
    "        ax[j].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each choice of compression algorithm, we compare different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncomp, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "    \n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plotlabel = f\"Chunk size {chunk_sizes[start+j*n]}\"\n",
    "        plottitle = f\"Compression: {compression[start]}\"\n",
    "        ax[i].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=plotlabel)\n",
    "        ax[i].legend(loc='upper left')\n",
    "        ax[i].set_title(plottitle)\n",
    "        ax[i].set_xticks(num_transactions[:n])\n",
    "        ax[i].set_xticklabels(num_transactions[:n])\n",
    "        ax[i].set_yticks(filesizes[selected])\n",
    "        ax[i].set_yticklabels(sizelabels[selected])\n",
    "        ax[i].grid(True)\n",
    "        ax[i].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_write = np.array([test['t_write'][-1] for test in testcase_4])\n",
    "fig_large_fraction_changes_times = plt.figure(figsize=(10,8))\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write[start+j*n:start+(j+1)*n], \n",
    "                 'o--', ms=8, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "t_write_nv = np.array([test['t_write'][-1] for test in testcase_4_no_versions])\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write_nv[start+j*n:start+(j+1)*n], \n",
    "                 '*-', ms=12, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}, No versioning\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(f\"{testname} - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions[:n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the graphs below show, for each fixed number of transactions, the time required to add new versions as the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_times, ax = plt.subplots(n+1, figsize=(14,20))\n",
    "fig_times.suptitle(f\"{testname}: time to write each new version\")\n",
    "\n",
    "for i in range(n):\n",
    "    for test in testcase_4:\n",
    "        if test['num_transactions'] == num_transactions[i]:\n",
    "            t_write = np.array(test['t_write'][:-1])\n",
    "            ax[i].plot(t_write, \n",
    "                       label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "            ax[i].legend(loc='upper left')\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "for test in testcase_4_no_versions:\n",
    "    if test['num_transactions'] == num_transactions[i]:\n",
    "        t_write = np.array(test['t_write'][:-1])\n",
    "        ax[n].plot(t_write, \n",
    "                   label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "        ax[n].legend(loc='upper left')\n",
    "        ax[n].set_title('No versioning')\n",
    "            \n",
    "plt.xlabel(\"Number of transactions\")\n",
    "plt.ylabel(\"Time (in seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behaviour is similar to what we observed in `mostly_appends_sparse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test5'></a>\n",
    "\n",
    "# Test 5: Large fraction changes - constant array size (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = \"test_large_fraction_constant_sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tested the following numbers of versions (or transactions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_transactions_5 = [50, 100, 500, 1000, 5000, 10000]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `num_transactions_5`, `exponents_5` and `compression_5` as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_5 = [50, 100, 500]\n",
    "exponents_5 = [12, 14]\n",
    "compression_5 = [None, \"gzip\", \"lzf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = performance_tests.test_large_fraction_constant_sparse(path=path,\n",
    "                                                                 num_transactions=num_transactions_5, \n",
    "                                                                 exponents=exponents_5, \n",
    "                                                                 compression=compression_5)\n",
    "\n",
    "testcase_5, msg = testcase.create_files()\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison, if you wish to generate the same tests with **no versioning**, meaning no use of the `VersionedHDF5` library, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_5_no_versions, msg = testcase.create_files(versions=False)\n",
    "if msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the results for later, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcase.save(testcase_5, f\"{testname}\")\n",
    "#testcase.save(testcase_5_no_versions, f\"{testname}_no_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading previously computed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an existing `.json` file, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{testname}.json\", \"r\") as json_in:\n",
    "    testcase_5 = json.load(json_in)\n",
    "with open(f\"{testname}_no_versions.json\", \"r\") as json_in:\n",
    "    testcase_5_no_versions = json.load(json_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's show the size information in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions = [test['num_transactions'] for test in testcase_5]\n",
    "chunk_sizes = [test['chunk_size'] for test in testcase_5]\n",
    "compression = [test['compression'] for test in testcase_5]\n",
    "filesizes = np.array([test['size'] for test in testcase_5])\n",
    "sizelabels = np.array([test['size_label'] for test in testcase_5])\n",
    "\n",
    "n = len(set(num_transactions))\n",
    "ncs = len(set(chunk_sizes))\n",
    "ncomp = len(set(compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, on the left we can see a linear plot, and on the right a loglog plot of the same size data for `testcase_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "\n",
    "selected = [0, 5, 8, 10, 11]\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[0].plot(num_transactions[:n],\n",
    "                   filesizes[start+j*n:start+(j+1)*n], \n",
    "                   '*--', ms=12, \n",
    "                   label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[1].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n], \n",
    "                     '*--', ms=12, \n",
    "                     label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "        ax[0].legend(loc='upper left')\n",
    "        ax[1].legend(loc='upper left')\n",
    "        ax[0].minorticks_off()\n",
    "        ax[1].minorticks_off()\n",
    "        # Changing the indices in selected will change the y-axis ticks in the graph for better visualization\n",
    "        ax[0].set_xticks(num_transactions[:n])\n",
    "        ax[0].set_xticklabels(num_transactions[:n])\n",
    "        ax[0].set_yticks(filesizes[selected])\n",
    "        ax[0].set_yticklabels(sizelabels[selected])\n",
    "        ax[0].set_xlabel(\"Transactions\")\n",
    "        ax[0].grid(False)\n",
    "        ax[1].set_xticks(num_transactions[:n])\n",
    "        ax[1].set_xticklabels(num_transactions[:n])\n",
    "        ax[1].set_yticks(filesizes[selected])\n",
    "        ax[1].set_yticklabels(sizelabels[selected])\n",
    "        ax[1].set_xlabel(\"Transactions\")\n",
    "        ax[1].grid(False)\n",
    "\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing compression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chunk size that we chose to test, let's compare the file sizes corresponding to each compression algorithm that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncs, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        ax[j].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=f\"{compression[start]}\")\n",
    "        ax[j].legend(loc='upper left')\n",
    "        ax[j].set_title(f\"Chunk Size {chunk_sizes[start+j*n]}\")\n",
    "        ax[j].set_xticks(num_transactions[:n])\n",
    "        ax[j].set_xticklabels(num_transactions[:n])\n",
    "        ax[j].set_yticks(filesizes[selected])\n",
    "        ax[j].set_yticklabels(sizelabels[selected])\n",
    "        ax[j].grid(True)\n",
    "        ax[j].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each choice of compression algorithm, we compare different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncomp, figsize=(10,10), sharey=True)\n",
    "fig.suptitle(f\"{testname}: File sizes\")\n",
    "    \n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plotlabel = f\"Chunk size {chunk_sizes[start+j*n]}\"\n",
    "        plottitle = f\"Compression: {compression[start]}\"\n",
    "        ax[i].loglog(num_transactions[:n],\n",
    "                     filesizes[start+j*n:start+(j+1)*n],\n",
    "                     '*--', ms=12, \n",
    "                     label=plotlabel)\n",
    "        ax[i].legend(loc='upper left')\n",
    "        ax[i].set_title(plottitle)\n",
    "        ax[i].set_xticks(num_transactions[:n])\n",
    "        ax[i].set_xticklabels(num_transactions[:n])\n",
    "        ax[i].set_yticks(filesizes[selected])\n",
    "        ax[i].set_yticklabels(sizelabels[selected])\n",
    "        ax[i].grid(True)\n",
    "        ax[i].minorticks_off()\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.suptitle(f\"{testname}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the creation times for these files, we have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_write = np.array([test['t_write'][-1] for test in testcase_5])\n",
    "fig_large_fraction_changes_times = plt.figure(figsize=(10,8))\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write[start+j*n:start+(j+1)*n], \n",
    "                 'o--', ms=8, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}\")\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "t_write_nv = np.array([test['t_write'][-1] for test in testcase_5_no_versions])\n",
    "for i in range(ncomp):\n",
    "    start = i*ncs*n\n",
    "    for j in range(ncs):\n",
    "        plt.plot(num_transactions[:n], \n",
    "                 t_write_nv[start+j*n:start+(j+1)*n], \n",
    "                 '*-', ms=12, \n",
    "                 label=f\"Chunk size {chunk_sizes[start+j*n]}, {compression[start]}, No versioning\")\n",
    "\n",
    "plt.xlabel(\"Transactions\")\n",
    "plt.title(f\"{testname} - creation times in seconds\")\n",
    "plt.legend()\n",
    "plt.xticks(num_transactions[:n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the graphs below show, for each fixed number of transactions, the time required to add new versions as the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_times, ax = plt.subplots(n+1, figsize=(14,20))\n",
    "fig_times.suptitle(f\"{testname}: time to write each new version\")\n",
    "\n",
    "for i in range(n):\n",
    "    for test in testcase_5:\n",
    "        if test['num_transactions'] == num_transactions[i]:\n",
    "            t_write = np.array(test['t_write'][:-1])\n",
    "            ax[i].plot(t_write, \n",
    "                       label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "            ax[i].legend(loc='upper left')\n",
    "\n",
    "# If you also with to plot information about the \"no versions\" test,  \n",
    "# run the following lines:\n",
    "for test in testcase_5_no_versions:\n",
    "    if test['num_transactions'] == num_transactions[i]:\n",
    "        t_write = np.array(test['t_write'][:-1])\n",
    "        ax[n].plot(t_write, \n",
    "                   label=f\"chunk size {test['chunk_size']}, {test['compression']}\")\n",
    "        ax[n].legend(loc='upper left')\n",
    "        ax[n].set_title('No versioning')\n",
    "            \n",
    "plt.xlabel(\"Number of transactions\")\n",
    "plt.ylabel(\"Time (in seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behaviour is unique to this case; here we can see the creation times do not vary as much between the versioned and non-versioned case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each versioned HDF5 file contains 3 datasets per version:\n",
    "- `key0`, an array of `int64`\n",
    "- `key1`, an array of `int64`\n",
    "- `val`, an array of `float64`\n",
    "plus metadata about groups, datasets and versions.\n",
    "\n",
    "This means that each file has  \n",
    "\n",
    "```\n",
    "nversions * 24 * arraysize + metadata\n",
    "```\n",
    "bytes of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standard'></a>\n",
    "## Standard parameters\n",
    "\n",
    "- `test_large_fraction_changes_sparse`: \n",
    "    - `num_rows_initial = 5000`\n",
    "    - `num_rows_per_append = 10`\n",
    "    - `num_inserts = 10`\n",
    "    - `num_deletes = 10`\n",
    "    - `num_changes = 1000`\n",
    "- `test_small_fraction_changes_sparse`\n",
    "    - `num_rows_initial = 5000`\n",
    "    - `num_rows_per_append = 10`\n",
    "    - `num_inserts = 10`\n",
    "    - `num_deletes = 10`\n",
    "    - `num_changes = 10`\n",
    "- `test_mostly_appends_sparse`:\n",
    "    - `num_rows_initial = 1000`\n",
    "    - `num_rows_per_append = 1000`\n",
    "    - `num_inserts = 10`\n",
    "    - `num_deletes = 10`\n",
    "    - `num_changes = 10`  \n",
    "- `test_mostly_appends_dense`\n",
    "    - `num_rows_initial_0 = 30`\n",
    "    - `num_rows_initial_1 = 30`\n",
    "    - `num_rows_per_append_0 = 1`\n",
    "    - `num_inserts_0 = 1`\n",
    "    - `num_inserts_1 = 10`\n",
    "    - `num_deletes_0 = 1`\n",
    "    - `num_deletes_1 = 1`\n",
    "    - `num_changes = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
